wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: mmw243 (fish-benchmark) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in ./logs/wandb/run-20250420_231922-auh3ox1e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-durian-21
wandb: â­ï¸ View project at https://wandb.ai/fish-benchmark/AbbyFrames_training
wandb: ğŸš€ View run at https://wandb.ai/fish-benchmark/AbbyFrames_training/runs/auh3ox1e
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
/home/mmw243/.conda/envs/clean/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.
You are using a CUDA device ('NVIDIA RTX 6000 Ada Generation') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
/home/mmw243/.conda/envs/clean/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type            | Params | Mode 
--------------------------------------------------
0 | model | MediaClassifier | 87.0 M | train
--------------------------------------------------
394 K     Trainable params
86.6 M    Non-trainable params
87.0 M    Total params
347.901   Total estimated model params size (MB)
5         Modules in train mode
224       Modules in eval mode
SLURM auto-requeueing enabled. Setting signal handlers.
/home/mmw243/.conda/envs/clean/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:384: `ModelCheckpoint(monitor='val_loss')` could not find the monitored key in the returned metrics: ['train_loss', 'train_recall', 'train_precision', 'train_f1', 'train_acc', 'train_class_0_pos_count', 'train_class_1_pos_count', 'train_class_0_pos_pred_count', 'train_class_1_pos_pred_count', 'train_class_0_accuracy', 'train_class_1_accuracy', 'epoch', 'step']. HINT: Did you call `log('val_loss', value)` in the `LightningModule`?
`Trainer.fit` stopped: `max_epochs=1` reached.
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                        epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    train_acc â–‡â–ˆâ–ˆâ–ƒâ–ƒâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–…â–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–‡â–†â–†â–†â–ˆâ–ˆâ–ˆâ–â–†
wandb:       train_class_0_accuracy â–ˆâ–†â–†â–ˆâ–„â–‡â–‡â–‡â–…â–ˆâ–ˆâ–ˆâ–ƒâ–ƒâ–ƒâ–†â–†â–†â–ˆâ–„â–ˆâ–ˆâ–‚â–†â–†â–†â–ˆâ–‡â–‡â–†â–‡â–…â–†â–ƒâ–ˆâ–…â–‡â–‡â–â–…
wandb:      train_class_0_pos_count â–ˆâ–â–‚â–‚â–‡â–â–‚â–†â–â–â–â–â–â–â–â–â–â–â–…â–â–‚â–ƒâ–‚â–â–‚â–â–â–â–â–ƒâ–‚â–â–…â–„â–â–â–â–â–…â–
wandb: train_class_0_pos_pred_count â–‡â–‚â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–â–â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–ƒâ–â–â–‚â–â–â–â–
wandb:       train_class_1_accuracy â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–…â–…â–ˆâ–ˆâ–ˆâ–ˆ
wandb:      train_class_1_pos_count â–â–â–â–â–â–â–â–â–â–â–‡â–â–â–â–â–‚â–â–â–ˆâ–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚
wandb: train_class_1_pos_pred_count â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                     train_f1 â–ƒâ–â–â–‚â–‚â–‚â–…â–…â–„â–„â–„â–„â–„â–„â–…â–…â–†â–†â–„â–„â–ƒâ–„â–‚â–…â–…â–ˆâ–ˆâ–‡â–ˆâ–…â–‡â–ƒâ–†â–ˆâ–ˆâ–…â–…â–…â–…â–…
wandb:                   train_loss â–â–â–â–â–ƒâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–â–â–â–ˆâ–„â–‚â–â–â–‚â–â–‚â–â–‚â–‚â–‚â–â–ƒâ–â–â–â–
wandb:              train_precision â–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–ˆâ–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ƒâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–„â–„â–ƒâ–ƒâ–ƒâ–†
wandb:                 train_recall â–ˆâ–ˆâ–‚â–‚â–â–â–‚â–‚â–„â–„â–ˆâ–…â–…â–…â–‚â–ˆâ–‚â–‚â–‡â–ˆâ–„â–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–‚â–…â–„â–„â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ƒ
wandb:          trainer/global_step â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:                        epoch 0
wandb:                    train_acc 0.92188
wandb:       train_class_0_accuracy 0.84375
wandb:      train_class_0_pos_count 18
wandb: train_class_0_pos_pred_count 23
wandb:       train_class_1_accuracy 0.96875
wandb:      train_class_1_pos_count 0
wandb: train_class_1_pos_pred_count 0
wandb:                     train_f1 0.87805
wandb:                   train_loss 0.12766
wandb:              train_precision 0.78261
wandb:                 train_recall 1
wandb:          trainer/global_step 2959
wandb: 
wandb: ğŸš€ View run expert-durian-21 at: https://wandb.ai/fish-benchmark/AbbyFrames_training/runs/auh3ox1e
wandb: â­ï¸ View project at: https://wandb.ai/fish-benchmark/AbbyFrames_training
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./logs/wandb/run-20250420_231922-auh3ox1e/logs
