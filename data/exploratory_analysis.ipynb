{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "config = yaml.safe_load(open(\"../config/datasets.yml\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"AbbySlidingWindow\"\n",
    "DATASET_DIR = config[DATASET][\"path\"]\n",
    "print(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fish_benchmark.utils import get_files_of_type\n",
    "print(get_files_of_type(\"/share/j_sun/jth264/abby/test/GX017042_clips_and_labels\", \".txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fish_benchmark.data.dataset import get_dataset\n",
    "dataset = get_dataset(\n",
    "    DATASET,\n",
    "    DATASET_DIR, \n",
    "    model_name=\"videomae\", \n",
    "    train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(13032/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for frame, label in tqdm(dataset):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame, label = next(iter(dataset))\n",
    "print(frame.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fish_benchmark.utils import get_files_of_type\n",
    "import numpy as np\n",
    "annotation_files = get_files_of_type(DATASET_DIR, \".txt\")\n",
    "\n",
    "annotations = []\n",
    "for file in annotation_files:\n",
    "    annotations.append(np.loadtxt(file, delimiter=\"\\t\"))\n",
    "\n",
    "annotations = np.concatenate(annotations, axis=0)\n",
    "annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fish_benchmark.data.dataset import AbbyDataset\n",
    "from fish_benchmark.models import get_input_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_transform = get_input_transform(\"videomae\")\n",
    "dataset = AbbyDataset(\n",
    "        DATASET_DIR, \n",
    "        train=True, \n",
    "        transform=input_transform, \n",
    "        label_type='onehot', \n",
    "        window_size=16, \n",
    "        tolerance_region = 7,\n",
    "        samples_per_window = 16,\n",
    "        step_size=1, \n",
    "        is_image_dataset=False\n",
    ")\n",
    "\n",
    "frame, label = next(iter(dataset))\n",
    "from fish_benchmark.debug import serialized_size\n",
    "print(serialized_size(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "labels = []\n",
    "for frame, label in tqdm(dataset):\n",
    "    labels.append(label)\n",
    "\n",
    "labels = torch.stack(labels)\n",
    "print(labels.sum(axis=0)/labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch \n",
    "from fish_benchmark.debug import serialized_size\n",
    "x = torch.randn(16, 3, 224, 224)\n",
    "start = time.time()\n",
    "torch.save(x, \"/share/j_sun/jth264/test.pt\")\n",
    "print(\"Time:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clips = [torch.randn(16, 3, 224, 224) for _ in range(100)]\n",
    "stacked = torch.stack(clips)\n",
    "clip0 = stacked[0]\n",
    "\n",
    "print(clip0.storage().size())          # Very large! Entire tensor storage\n",
    "print(serialized_size(clip0))          # Very large! ~1GB, same as full tensor\n",
    "print(clip0.clone().storage().size())  # Just right! ~9MB worth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
