{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "config = yaml.safe_load(open(\"../config/datasets.yml\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"AbbySlidingWindow\"\n",
    "DATASET_DIR = config[DATASET][\"path\"]\n",
    "print(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fish_benchmark.utils import get_files_of_type\n",
    "import numpy as np\n",
    "annotation_files = get_files_of_type(DATASET_DIR, \".txt\")\n",
    "\n",
    "annotations = []\n",
    "for file in annotation_files:\n",
    "    annotations.append(np.loadtxt(file, delimiter=\"\\t\"))\n",
    "\n",
    "annotations = np.concatenate(annotations, axis=0)\n",
    "annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fish_benchmark.data.dataset import AbbyDataset\n",
    "from fish_benchmark.models import get_input_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for frame, label in tqdm(dataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1021183132\n"
     ]
    }
   ],
   "source": [
    "input_transform = get_input_transform(\"videomae\")\n",
    "dataset = AbbyDataset(\n",
    "        DATASET_DIR, \n",
    "        train=True, \n",
    "        transform=input_transform, \n",
    "        label_type='onehot', \n",
    "        window_size=16, \n",
    "        tolerance_region = 7,\n",
    "        samples_per_window = 16,\n",
    "        step_size=1, \n",
    "        is_image_dataset=False\n",
    ")\n",
    "\n",
    "frame, label = next(iter(dataset))\n",
    "from fish_benchmark.debug import serialized_size\n",
    "print(serialized_size(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "106it [00:06, 15.40it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      4\u001b[39m dataset = AbbyDataset(\n\u001b[32m      5\u001b[39m         DATASET_DIR, \n\u001b[32m      6\u001b[39m         train=\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m         is_image_dataset=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m labels = []\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m labels = torch.stack(labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/benchmark/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fish-benchmark/fish_benchmark/data/dataset.py:320\u001b[39m, in \u001b[36mAbbyDataset.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, frame \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(container.decode(video=\u001b[32m0\u001b[39m)):\n\u001b[32m    318\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m frame.to_image(), label[i]\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_video_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotated_frame_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m clip, label \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m clip, label\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fish-benchmark/fish_benchmark/data/dataset.py:224\u001b[39m, in \u001b[36mBaseSlidingWindowDataset.create_video_dataset\u001b[39m\u001b[34m(self, annotated_video_frames)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m#turn clip into a tensor\u001b[39;00m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.input_transform:\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     clip = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[32m    226\u001b[39m     clip = torch.from_numpy(clip)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fish-benchmark/fish_benchmark/models.py:79\u001b[39m, in \u001b[36mget_input_transform.<locals>.<lambda>\u001b[39m\u001b[34m(video)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m model_name == \u001b[33m'\u001b[39m\u001b[33mvideomae\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     78\u001b[39m     processor = AutoImageProcessor.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mMCG-NJU/videomae-base\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     transform = \u001b[38;5;28;01mlambda\u001b[39;00m video: \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.pixel_values.squeeze(\u001b[32m0\u001b[39m)\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m transform\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m model_name == \u001b[33m'\u001b[39m\u001b[33mswinv2\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/benchmark/lib/python3.12/site-packages/transformers/image_processing_utils.py:42\u001b[39m, in \u001b[36mBaseImageProcessor.__call__\u001b[39m\u001b[34m(self, images, **kwargs)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, **kwargs) -> BatchFeature:\n\u001b[32m     41\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/benchmark/lib/python3.12/site-packages/transformers/utils/generic.py:854\u001b[39m, in \u001b[36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m         cls_prefix = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    847\u001b[39m     warnings.warn(\n\u001b[32m    848\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    849\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    850\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    851\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    852\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m854\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalid_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/benchmark/lib/python3.12/site-packages/transformers/models/videomae/image_processing_videomae.py:324\u001b[39m, in \u001b[36mVideoMAEImageProcessor.preprocess\u001b[39m\u001b[34m(self, videos, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format)\u001b[39m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    316\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    317\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtorch.Tensor, tf.Tensor or jax.ndarray.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    318\u001b[39m     )\n\u001b[32m    320\u001b[39m videos = make_batched(videos)\n\u001b[32m    322\u001b[39m videos = [\n\u001b[32m    323\u001b[39m     [\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_preprocess_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m            \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdo_resize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_resize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m            \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdo_center_crop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_center_crop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcrop_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcrop_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrescale_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrescale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdo_normalize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_normalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m            \u001b[49m\u001b[43mimage_mean\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m            \u001b[49m\u001b[43mimage_std\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m video\n\u001b[32m    340\u001b[39m     ]\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m video \u001b[38;5;129;01min\u001b[39;00m videos\n\u001b[32m    342\u001b[39m ]\n\u001b[32m    344\u001b[39m data = {\u001b[33m\"\u001b[39m\u001b[33mpixel_values\u001b[39m\u001b[33m\"\u001b[39m: videos}\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BatchFeature(data=data, tensor_type=return_tensors)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/benchmark/lib/python3.12/site-packages/transformers/models/videomae/image_processing_videomae.py:221\u001b[39m, in \u001b[36mVideoMAEImageProcessor._preprocess_image\u001b[39m\u001b[34m(self, image, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, data_format, input_data_format)\u001b[39m\n\u001b[32m    218\u001b[39m     input_data_format = infer_channel_dimension_format(image)\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_center_crop:\n\u001b[32m    224\u001b[39m     image = \u001b[38;5;28mself\u001b[39m.center_crop(image, size=crop_size, input_data_format=input_data_format)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/benchmark/lib/python3.12/site-packages/transformers/models/videomae/image_processing_videomae.py:169\u001b[39m, in \u001b[36mVideoMAEImageProcessor.resize\u001b[39m\u001b[34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSize must have \u001b[39m\u001b[33m'\u001b[39m\u001b[33mheight\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mwidth\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mshortest_edge\u001b[39m\u001b[33m'\u001b[39m\u001b[33m as keys. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize.keys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/benchmark/lib/python3.12/site-packages/transformers/image_transforms.py:373\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[39m\n\u001b[32m    371\u001b[39m height, width = size\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# PIL images are in the format (width, height)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m resized_image = \u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreducing_gap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreducing_gap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_numpy:\n\u001b[32m    376\u001b[39m     resized_image = np.array(resized_image)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/benchmark/lib/python3.12/site-packages/PIL/Image.py:2365\u001b[39m, in \u001b[36mImage.resize\u001b[39m\u001b[34m(self, size, resample, box, reducing_gap)\u001b[39m\n\u001b[32m   2353\u001b[39m         \u001b[38;5;28mself\u001b[39m = (\n\u001b[32m   2354\u001b[39m             \u001b[38;5;28mself\u001b[39m.reduce(factor, box=reduce_box)\n\u001b[32m   2355\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.reduce)\n\u001b[32m   2356\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m Image.reduce(\u001b[38;5;28mself\u001b[39m, factor, box=reduce_box)\n\u001b[32m   2357\u001b[39m         )\n\u001b[32m   2358\u001b[39m         box = (\n\u001b[32m   2359\u001b[39m             (box[\u001b[32m0\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2360\u001b[39m             (box[\u001b[32m1\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2361\u001b[39m             (box[\u001b[32m2\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2362\u001b[39m             (box[\u001b[32m3\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2363\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2365\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "labels = []\n",
    "for frame, label in tqdm(dataset):\n",
    "    labels.append(label)\n",
    "\n",
    "labels = torch.stack(labels)\n",
    "print(labels.sum(axis=0)/labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.017487525939941406\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch \n",
    "from fish_benchmark.debug import serialized_size\n",
    "x = torch.randn(16, 3, 224, 224)\n",
    "start = time.time()\n",
    "torch.save(x, \"/share/j_sun/jth264/test.pt\")\n",
    "print(\"Time:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240844800\n",
      "963380380\n",
      "2408448\n"
     ]
    }
   ],
   "source": [
    "clips = [torch.randn(16, 3, 224, 224) for _ in range(100)]\n",
    "stacked = torch.stack(clips)\n",
    "clip0 = stacked[0]\n",
    "\n",
    "print(clip0.storage().size())          # Very large! Entire tensor storage\n",
    "print(serialized_size(clip0))          # Very large! ~1GB, same as full tensor\n",
    "print(clip0.clone().storage().size())  # Just right! ~9MB worth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
