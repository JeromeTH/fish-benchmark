{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "config = yaml.safe_load(open(\"../config/datasets.yml\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"AbbySlidingWindow\"\n",
    "DATASET_DIR = config[DATASET][\"path\"]\n",
    "print(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fish_benchmark.utils import get_files_of_type\n",
    "print(get_files_of_type(\"/share/j_sun/jth264/abby/test/GX017042_clips_and_labels\", \".txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fish_benchmark.data.dataset import get_dataset\n",
    "dataset = get_dataset(\n",
    "    DATASET,\n",
    "    DATASET_DIR, \n",
    "    model_name=\"videomae\", \n",
    "    train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(13032/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for frame, label in tqdm(dataset):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame, label = next(iter(dataset))\n",
    "print(frame.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fish_benchmark.utils import get_files_of_type\n",
    "import numpy as np\n",
    "annotation_files = get_files_of_type(DATASET_DIR, \".txt\")\n",
    "\n",
    "annotations = []\n",
    "for file in annotation_files:\n",
    "    annotations.append(np.loadtxt(file, delimiter=\"\\t\"))\n",
    "\n",
    "annotations = np.concatenate(annotations, axis=0)\n",
    "annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fish_benchmark.data.dataset import AbbyDataset\n",
    "from fish_benchmark.models import get_input_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_transform = get_input_transform(\"videomae\")\n",
    "dataset = AbbyDataset(\n",
    "        DATASET_DIR, \n",
    "        train=True, \n",
    "        transform=input_transform, \n",
    "        label_type='onehot', \n",
    "        window_size=16, \n",
    "        tolerance_region = 7,\n",
    "        samples_per_window = 16,\n",
    "        step_size=1, \n",
    "        is_image_dataset=False\n",
    ")\n",
    "\n",
    "frame, label = next(iter(dataset))\n",
    "from fish_benchmark.debug import serialized_size\n",
    "print(serialized_size(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "labels = []\n",
    "for frame, label in tqdm(dataset):\n",
    "    labels.append(label)\n",
    "\n",
    "labels = torch.stack(labels)\n",
    "print(labels.sum(axis=0)/labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch \n",
    "from fish_benchmark.debug import serialized_size\n",
    "x = torch.randn(16, 3, 224, 224)\n",
    "start = time.time()\n",
    "torch.save(x, \"/share/j_sun/jth264/test.pt\")\n",
    "print(\"Time:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clips = [torch.randn(16, 3, 224, 224) for _ in range(100)]\n",
    "stacked = torch.stack(clips)\n",
    "clip0 = stacked[0]\n",
    "\n",
    "print(clip0.storage().size())          # Very large! Entire tensor storage\n",
    "print(serialized_size(clip0))          # Very large! ~1GB, same as full tensor\n",
    "print(clip0.clone().storage().size())  # Just right! ~9MB worth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jth264/.conda/envs/benchmark/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from fish_benchmark.data.preprocessors import TorchVisionPreprocessor\n",
    "from fish_benchmark.models import get_input_transform\n",
    "from fish_benchmark.data.dataset import get_dataset\n",
    "img_tensor = torch.randint(0, 256, (3, 480, 640), dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from fish_benchmark.data.dataset import MikeDataset\n",
    "DATASET = \"AbbySlidingWindow\"\n",
    "MODEL =  None # \"multipatch_dino\"\n",
    "config = yaml.safe_load(open(\"../config/datasets.yml\", \"r\"))\n",
    "input_transform = get_input_transform(MODEL) if MODEL else None\n",
    "dataset = get_dataset(\n",
    "    DATASET, \n",
    "    path = \"/share/j_sun/jth264/abby/train/GX017102_annotations\", \n",
    "    augs=input_transform,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00000000..00000999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00001000..00001999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00002000..00002999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00003000..00003999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00004000..00004999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00005000..00005999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00006000..00006999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00007000..00007999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00008000..00008999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00009000..00009999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00010000..00010999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00011000..00011999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00012000..00012999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00013000..00013999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00014000..00014999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00015000..00015999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00016000..00016999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00017000..00017999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00018000..00018999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00019000..00019999.tar', '/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367/AT_070523_GH010367-00020000..00020133.tar']\n"
     ]
    }
   ],
   "source": [
    "from fish_benchmark.utils import get_files_of_type\n",
    "print(get_files_of_type(\"/share/j_sun/jth264/bites_frame_annotation_splitted/train/AT_070523_GH010367\", \".tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for clip, label in tqdm(dataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip, label = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 220, 220])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_example = torch.rand(4, 3, 760, 1352)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torch\n",
    "from fish_benchmark.debug import step_timer\n",
    "\n",
    "class TorchVisionPreprocessor:\n",
    "    def __init__(self, crop_size=(224, 224), resize_shortest=256,\n",
    "                 mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225),\n",
    "                 interpolation=InterpolationMode.BICUBIC):\n",
    "\n",
    "        self.resize = v2.Resize(resize_shortest, interpolation=interpolation, antialias=False)\n",
    "        self.crop = v2.CenterCrop(crop_size)\n",
    "        self.mean = torch.tensor(mean).view(3, 1, 1)\n",
    "        self.std = torch.tensor(std).view(3, 1, 1)\n",
    "\n",
    "    def __call__(self, image_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        with step_timer(\"resize\"):\n",
    "            image_tensor = self.resize(image_tensor)\n",
    "        with step_timer(\"crop\"):\n",
    "            image_tensor = self.crop(image_tensor)\n",
    "\n",
    "        with step_timer(\"normalize\"):\n",
    "            image_tensor = (image_tensor - self.mean) / self.std\n",
    "            #image_tensor = self.normalize(image_tensor)\n",
    "\n",
    "        return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[resize] took 0.021542 seconds\n",
      "[crop] took 0.000070 seconds\n",
      "[normalize] took 0.007196 seconds\n",
      "torch.Size([16, 3, 224, 224])\n",
      "[Preprocess] took 0.029095 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preprocessor = TorchVisionPreprocessor()\n",
    "with step_timer(\"Preprocess\"):\n",
    "    print(preprocessor(clip).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
