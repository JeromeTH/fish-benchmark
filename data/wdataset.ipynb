{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import webdataset as wds\n",
    "path = \"/share/j_sun/jth264/bites_frame_annotation\" \n",
    "# dataset = \"SR_070723_GH030275-000000..000099.tar\"\n",
    "# url = os.path.join(path, dataset)\n",
    "tar_files = [os.path.join(path, tarfile) for tarfile in os.listdir(path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fish_benchmark.data.dataset import HeinFishBehavior\n",
    "dataset = HeinFishBehavior(tar_files=tar_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(dataset))\n",
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "processed_image = processor(sample[0], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_image.pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_image\n",
    "from torchvision.transforms import ToPILImage\n",
    "to_pil = ToPILImage()\n",
    "img_pil = to_pil(processed_image['pixel_values'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[1]\n",
    "categories = dataset.categories\n",
    "for label in sample[1]:\n",
    "    print(categories[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, sample in enumerate(dataset):\n",
    "    if id % 100 == 0:\n",
    "        print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.DataPipeline(\n",
    "        wds.SimpleShardList(tar_files),\n",
    "        wds.decode(\"pil\"),\n",
    "        wds.to_tuple(\"png\", \"json\")\n",
    "    )\n",
    "dataset = wds.WebDataset(tar_files).decode(\"pil\").to_tuple(\"png\", \"json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dataset:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = sample[1]\n",
    "\n",
    "for event in annotation['events']:\n",
    "    print(event['behavior']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check what is inside the tar file\n",
    "import tarfile\n",
    "with tarfile.open(url, 'r') as tar:\n",
    "    tar.extractall(path=path)\n",
    "    print(tar.getnames())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"SR_070723_GH030275_000033.png\"\n",
    "from PIL import Image\n",
    "image = Image.open(os.path.join(path, image_path))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = [i for i in range (10)]\n",
    "with open(\"data.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=1)\n",
    "\n",
    "with open(\"data.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Caltech101\n",
    "\n",
    "dataset = Caltech101(root=\".\", target_type = \"category\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(1, 'tensor.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torch.load('tensor.pt')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from fish_benchmark.models import get_input_transform\n",
    "\n",
    "transform = get_input_transform(\"dino\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 224, 224])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = torch.rand(16, 3, 1500, 1500)\n",
    "transform(video).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fish_benchmark.data.dataset import HeinFishBehaviorSlidingWindow\n",
    "import yaml\n",
    "config = yaml.safe_load(open(\"../config/datasets.yml\")) \n",
    "DATASET = \"HeinFishBehaviorSlidingWindow\"\n",
    "dataset = HeinFishBehaviorSlidingWindow(\n",
    "    path = config[DATASET]['path']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/share/j_sun/jth264/bites_frame_annotation/ER_062623_GH010395/ER_062623_GH010395-00000000..00000999.tar', '/share/j_sun/jth264/bites_frame_annotation/ER_062623_GH010395/ER_062623_GH010395-00001000..00001999.tar', '/share/j_sun/jth264/bites_frame_annotation/ER_062623_GH010395/ER_062623_GH010395-00002000..00002999.tar', '/share/j_sun/jth264/bites_frame_annotation/ER_062623_GH010395/ER_062623_GH010395-00003000..00003999.tar', '/share/j_sun/jth264/bites_frame_annotation/ER_062623_GH010395/ER_062623_GH010395-00004000..00004999.tar', '/share/j_sun/jth264/bites_frame_annotation/ER_062623_GH010395/ER_062623_GH010395-00005000..00005999.tar', '/share/j_sun/jth264/bites_frame_annotation/ER_062623_GH010395/ER_062623_GH010395-00006000..00006999.tar', '/share/j_sun/jth264/bites_frame_annotation/ER_062623_GH010395/ER_062623_GH010395-00007000..00007999.tar', '/share/j_sun/jth264/bites_frame_annotation/ER_062623_GH010395/ER_062623_GH010395-00008000..00008999.tar', '/share/j_sun/jth264/bites_frame_annotation/ER_062623_GH010395/ER_062623_GH010395-00009000..00009999.tar', '/share/j_sun/jth264/bites_frame_annotation/ER_062623_GH010395/ER_062623_GH010395-00010000..00010999.tar', '/share/j_sun/jth264/bites_frame_annotation/ER_062623_GH010395/ER_062623_GH010395-00011000..00011999.tar', '/share/j_sun/jth264/bites_frame_annotation/ER_062623_GH010395/ER_062623_GH010395-00012000..00012999.tar', '/share/j_sun/jth264/bites_frame_annotation/ER_062623_GH010395/ER_062623_GH010395-00013000..00013999.tar', '/share/j_sun/jth264/bites_frame_annotation/ER_062623_GH010395/ER_062623_GH010395-00014000..00014999.tar', '/share/j_sun/jth264/bites_frame_annotation/ER_062623_GH010395/ER_062623_GH010395-00015000..00015999.tar', '/share/j_sun/jth264/bites_frame_annotation/ER_062623_GH010395/ER_062623_GH010395-00016000..00016999.tar', '/share/j_sun/jth264/bites_frame_annotation/ER_062623_GH010395/ER_062623_GH010395-00017000..00017999.tar', '/share/j_sun/jth264/bites_frame_annotation/ER_062623_GH010395/ER_062623_GH010395-00018000..00018072.tar']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jth264/.conda/envs/benchmark/lib/python3.12/site-packages/webdataset/compat.py:389: UserWarning: WebDataset(shardshuffle=...) is None; set explicitly to False or a number\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "video, annotation = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 1520, 2704, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 224, 224])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fish_benchmark.models import get_input_transform\n",
    "transform = get_input_transform(\"videomae\")\n",
    "transform(video).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1520, 2704])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fish_benchmark.data.dataset import HeinFishBehavior\n",
    "import yaml\n",
    "from fish_benchmark.models import get_input_transform\n",
    "transform = get_input_transform(\"swinv2\", do_resize=False)\n",
    "config = yaml.safe_load(open(\"../config/datasets.yml\")) \n",
    "DATASET = \"HeinFishBehavior\"\n",
    "dataset = HeinFishBehavior(\n",
    "    path = config[DATASET]['path'], \n",
    "    transform = transform \n",
    ")\n",
    "image, annotation = next(iter(dataset))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "749it [05:52,  2.12it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/benchmark/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fish-benchmark/fish_benchmark/data/dataset.py:176\u001b[39m, in \u001b[36mHeinFishBehavior.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    174\u001b[39m image, annotation = sample\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m labels = [\u001b[38;5;28mself\u001b[39m.behavior_idx_map[behavior] \u001b[38;5;28;01mfor\u001b[39;00m behavior \u001b[38;5;129;01min\u001b[39;00m parse_annotation(annotation)]\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.label_type == \u001b[33m'\u001b[39m\u001b[33monehot\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fish-benchmark/fish_benchmark/models.py:83\u001b[39m, in \u001b[36mget_input_transform.<locals>.<lambda>\u001b[39m\u001b[34m(img)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m model_name == \u001b[33m'\u001b[39m\u001b[33mswinv2\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     82\u001b[39m     image_processor = AutoImageProcessor.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mmicrosoft/swinv2-tiny-patch4-window8-256\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     transform = \u001b[38;5;28;01mlambda\u001b[39;00m img: \u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRGB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_resize\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_resize\u001b[49m\u001b[43m)\u001b[49m.pixel_values.squeeze(\u001b[32m0\u001b[39m)\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m transform\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m model_name == \u001b[33m'\u001b[39m\u001b[33mtimesformer\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/benchmark/lib/python3.12/site-packages/transformers/image_processing_utils.py:42\u001b[39m, in \u001b[36mBaseImageProcessor.__call__\u001b[39m\u001b[34m(self, images, **kwargs)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, **kwargs) -> BatchFeature:\n\u001b[32m     41\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/benchmark/lib/python3.12/site-packages/transformers/utils/generic.py:854\u001b[39m, in \u001b[36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m         cls_prefix = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    847\u001b[39m     warnings.warn(\n\u001b[32m    848\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    849\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    850\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    851\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    852\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m854\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalid_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/benchmark/lib/python3.12/site-packages/transformers/models/vit/image_processing_vit.py:274\u001b[39m, in \u001b[36mViTImageProcessor.preprocess\u001b[39m\u001b[34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format, do_convert_rgb)\u001b[39m\n\u001b[32m    267\u001b[39m     images = [\n\u001b[32m    268\u001b[39m         \u001b[38;5;28mself\u001b[39m.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n\u001b[32m    269\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[32m    270\u001b[39m     ]\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_normalize:\n\u001b[32m    273\u001b[39m     images = [\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m         \u001b[38;5;28mself\u001b[39m.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n\u001b[32m    275\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[32m    276\u001b[39m     ]\n\u001b[32m    278\u001b[39m images = [\n\u001b[32m    279\u001b[39m     to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[32m    280\u001b[39m ]\n\u001b[32m    282\u001b[39m data = {\u001b[33m\"\u001b[39m\u001b[33mpixel_values\u001b[39m\u001b[33m\"\u001b[39m: images}\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for id, (image, annotation) in tqdm(enumerate(dataset)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fish_benchmark.data.dataset import AbbyDataset\n",
    "import yaml\n",
    "config = yaml.safe_load(open(\"../config/datasets.yml\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"Abby\"\n",
    "dataset = AbbyDataset(\n",
    "    path = config[DATASET]['path'], \n",
    "    transform = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_1053.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_1053.txt\n"
     ]
    }
   ],
   "source": [
    "frame, label = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:00, 504.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_1053.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_1053.txt\n",
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_1178.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_1178.txt\n",
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_1256.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_1256.txt\n",
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_13.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_13.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "894it [00:00, 1615.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_1444.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_1444.txt\n",
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_1462.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_1462.txt\n",
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_1472.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_1472.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1059it [00:00, 1005.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_17.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_17.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1290it [00:01, 959.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_184.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_184.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2344it [00:01, 1680.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_2.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_2.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3055it [00:02, 1713.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_22.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_22.txt\n",
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_24.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_24.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4166it [00:02, 1906.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_336.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_336.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4630it [00:03, 1699.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_353.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_353.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5363it [00:03, 1936.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_364.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_364.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5830it [00:03, 1987.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_37.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_37.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6504it [00:03, 1721.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_431.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_431.txt\n",
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_48.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_48.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7191it [00:04, 1731.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_5.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_5.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7940it [00:04, 1912.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_507.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_507.txt\n",
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_53.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_53.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8680it [00:05, 1851.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_541.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_541.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9355it [00:05, 1782.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_657.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_657.txt\n",
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_661.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_661.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9546it [00:05, 1407.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_692.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_692.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10226it [00:06, 1449.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_700.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_700.txt\n",
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_717.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_717.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10670it [00:06, 1626.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_781.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_781.txt\n",
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_80.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_80.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11248it [00:06, 1820.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_839.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_839.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11681it [00:06, 1996.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_851.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_851.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12122it [00:07, 1938.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_910.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_910.txt\n",
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_93.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_93.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12809it [00:07, 1948.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_942.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_942.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13031it [00:07, 2020.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_953.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_953.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13239it [00:07, 1162.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_970.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_970.txt\n",
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_978.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_978.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13559it [00:08, 1673.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /share/j_sun/jth264/abby/GX017102_annotations/GX017102/track_998.mp4 and /share/j_sun/jth264/abby/GX017102_annotations/track_998.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for id, (frame, label) in tqdm(enumerate(dataset)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '0'], dtype='<U1')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
